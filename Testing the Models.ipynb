{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95042e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32ba0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
    "import torch\n",
    "import gc\n",
    "import accelerate\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
    "from huggingface_hub import login\n",
    "\n",
    "models_path = r\"../Models\"\n",
    "code_path = r\"../Code\"\n",
    "data_path = r\"../Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de5fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "file = (os.path.join(code_path,r'HF_key.tex'))\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "HF_key = lines[0].split(\"= \")[1]\n",
    "\n",
    "try:\n",
    "    login(token=HF_key)\n",
    "    print(\"Successfully logged in to Hugging Face Hub.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to Hugging Face Hub: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def2d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Custom Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147380",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def clear_pipeline(pipe, verbosity=0):\n",
    "    \"\"\"Clears a Hugging Face pipeline and frees CUDA memory.\"\"\"\n",
    "    if hasattr(pipe, \"model\") and next(pipe.model.parameters()).is_cuda:\n",
    "        initial_allocated = torch.cuda.memory_allocated() / 1e6\n",
    "        initial_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "\n",
    "        if verbosity > 0:\n",
    "            print(f\"üîç Before unloading: {initial_allocated:.2f} MB allocated, {initial_reserved:.2f} MB reserved.\")\n",
    "\n",
    "        try:\n",
    "            pipe.model.to(\"cpu\")\n",
    "            for param in pipe.model.parameters():\n",
    "                param.data = param.data.cpu()\n",
    "        except Exception as e:\n",
    "            if verbosity > 0:\n",
    "                print(f\"‚ö†Ô∏è Error moving model to CPU: {e}\")\n",
    "\n",
    "        del pipe.model\n",
    "        del pipe\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        final_allocated = torch.cuda.memory_allocated() / 1e6\n",
    "        final_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "\n",
    "        if verbosity > 0:\n",
    "            print(f\"‚úÖ Pipeline cleared. Freed {initial_allocated - final_allocated:.2f} MB allocated, \"\n",
    "                  f\"{initial_reserved - final_reserved:.2f} MB reserved.\")\n",
    "    else:\n",
    "        if verbosity > 0:\n",
    "            print(\"‚ÑπÔ∏è Pipeline already on CPU. Performing standard cleanup.\")\n",
    "        del pipe\n",
    "        gc.collect()\n",
    "\n",
    "    if verbosity > 0:\n",
    "        print(\"üóëÔ∏è Cleanup complete.\")\n",
    "    elif verbosity == 0:\n",
    "        print(\"‚úÖ Pipeline cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89633b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Text to be translated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1354bd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "hm_text = \"Koj puas mob taub hau?\"\n",
    "en_text = \"Do you have a headache?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b927",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Original t5\\-base\\-en2vi vs. Fine\\-Tuned Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29169f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Koj puas mob taub hau?']"
      ]
     },
     "execution_count": 39,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"trungnguyentran/t5-base-en2vi\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = ['Koj puas mob taub hau?']\n",
    "\n",
    "outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids, max_length=512)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f70594",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B·∫°n c√≥ b·ªã ƒëau ƒë·∫ßu kh√¥ng?']"
      ]
     },
     "execution_count": 42,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"trungnguyentran/t5-base-en2vi\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = ['Do you have a headache?']\n",
    "\n",
    "outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids, max_length=512)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11a386",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koj puas mob taub hau?\n",
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "translation_pipeline = pipeline(\"translation\", model=\"trungnguyentran/t5-base-en2vi\")\n",
    "translation_result = translation_pipeline(hm_text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n",
    "\n",
    "clear_pipeline(translation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7cf0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fine\\-tuned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9dec7f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sv taub hau']"
      ]
     },
     "execution_count": 47,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"shpie/t5-base-en2vi-finetuned-Hmong-to-English\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = ['Koj puas mob taub hau?']\n",
    "\n",
    "outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids, max_length=512)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bc019",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv taub hau\n",
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "translation_pipeline = pipeline(\"translation\", model=\"shpie/t5-base-en2vi-finetuned-Hmong-to-English\")\n",
    "translation_result = translation_pipeline(hm_text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n",
    "\n",
    "clear_pipeline(translation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af534b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o you you a teevneeg']"
      ]
     },
     "execution_count": 48,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"shpie/t5-base-en2vi-finetuned-English-to-Hmong\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = ['Do you have a headache?']\n",
    "\n",
    "outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids, max_length=512)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f490b8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o you you a teevneeg\n",
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "translation_pipeline = pipeline(\"translation\", model=\"shpie/t5-base-en2vi-finetuned-English-to-Hmong\")\n",
    "translation_result = translation_pipeline(en_text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n",
    "\n",
    "clear_pipeline(translation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c1ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Original ByT5\\-small vs. Fine\\-Tuned Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbbd6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8959, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\n",
    "\n",
    "input_ids = torch.tensor([list(\"I like to eat food.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"Kuv nyiam noj mov.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1cc3f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0459, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\n",
    "\n",
    "input_ids = torch.tensor([list(\"Do you have a headache?\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"Koj puas mob taub hau?\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f05e82",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4951, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\n",
    "\n",
    "input_ids = torch.tensor([list(\"Kuv nyiam noj mov.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"I like to eat food.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b51879",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0100, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\n",
    "\n",
    "input_ids = torch.tensor([list(\"Koj puas mob taub hau?\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"Do you have a headache?\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9329",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fine\\-Tuned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05ed8d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4951, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('shpie/byt5-small-finetuned-Hmong-to-English')\n",
    "\n",
    "input_ids = torch.tensor([list(\"Kuv nyiam noj mov.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"I like to eat food.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f806c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0100, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('shpie/byt5-small-finetuned-Hmong-to-English')\n",
    "\n",
    "input_ids = torch.tensor([list(\"Koj puas mob taub hau?\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"Do you have a headache?\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7182e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8959, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('shpie/byt5-small-finetuned-Hmong-to-English')\n",
    "\n",
    "input_ids = torch.tensor([list(\"I like to eat food.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"Kuv nyiam noj mov.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fe3b6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      "Taub hau?\n",
      " tau\n",
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "translation_pipeline = pipeline(\"translation\", model=\"shpie/byt5-small-finetuned-Hmong-to-English\", tokenizer=AutoTokenizer.from_pretrained(\"google/byt5-small\"))\n",
    "translation_result = translation_pipeline(hm_text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n",
    "\n",
    "clear_pipeline(translation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6950",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8959, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('shpie/byt5-small-finetuned-English-to-Hmong')\n",
    "\n",
    "input_ids = torch.tensor([list(\"I like to eat food.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"Kuv nyiam noj mov.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "789760",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0459, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('shpie/byt5-small-finetuned-English-to-Hmong')\n",
    "\n",
    "input_ids = torch.tensor([list(\"Do you have a headache?\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"Koj puas mob taub hau?\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e87aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4951, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('shpie/byt5-small-finetuned-English-to-Hmong')\n",
    "\n",
    "input_ids = torch.tensor([list(\"Kuv nyiam noj mov.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "labels = torch.tensor([list(\"I like to eat food.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "loss = model(input_ids, labels=labels).loss # forward pass\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed505e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave a headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "Headache?\n",
      "‚úÖ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "translation_pipeline = pipeline(\"translation\", model=\"shpie/byt5-small-finetuned-English-to-Hmong\", tokenizer=AutoTokenizer.from_pretrained(\"google/byt5-small\"))\n",
    "translation_result = translation_pipeline(en_text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n",
    "\n",
    "clear_pipeline(translation_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3.10.12",
   "env": {
   },
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
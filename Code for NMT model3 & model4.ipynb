{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c8d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee570c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import evaluate\n",
    "metric=evaluate.load(\"sacrebleu\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
    "from huggingface_hub import login\n",
    "\n",
    "models_path = r\"../Models\"\n",
    "code_path = r\"../Code\"\n",
    "data_path = r\"../Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8492ff",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "file = (os.path.join(code_path,r'HF_key.tex'))\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "HF_key = lines[0].split(\"= \")[1]\n",
    "\n",
    "try:\n",
    "    login(token=HF_key)\n",
    "    print(\"Successfully logged in to Hugging Face Hub.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to Hugging Face Hub: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd0ff",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "dataset = load_dataset(\"shpie/Hmong-to-Eng-4k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333231",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39aaba",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c96afd79512425cb8a80cba0257127f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1a64220d26465cb27fd6982b7b9970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e415ab86d044529cc2012ad55f610d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f0380",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True)\n",
    "\n",
    "source_lang = \"Hmong\"\n",
    "target_lang = \"English\"\n",
    "max_target_length=128\n",
    "max_input_length=128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[source_lang]]\n",
    "    targets = [ex for ex in examples[target_lang]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['Hmong', 'English'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db37a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [92, 120, 100, 121, 35, 119, 118, 120, 112, 35, 118, 100, 120, 35, 113, 119, 100, 122, 121, 35, 116, 107, 108, 100, 35, 102, 114, 121, 35, 113, 104, 104, 106, 35, 112, 114, 101, 35, 119, 123, 114, 106, 35, 110, 104, 121, 35, 113, 115, 100, 109, 35, 117, 107, 114, 35, 119, 100, 122, 112, 35, 111, 114, 118, 35, 118, 108, 118, 35, 107, 111, 114, 114, 121, 35, 102, 107, 100, 122, 35, 119, 107, 108, 100, 101, 35, 118, 104, 101, 35, 121, 108, 112, 35, 111, 108, 35, 102, 100, 118, 119, 35, 118, 108, 118, 35, 115, 120, 101, 35, 103, 107, 100, 120, 35, 111, 108, 35, 54, 51, 35, 107, 113, 120, 101, 35, 120, 100, 35, 113, 119, 104, 109, 35, 124, 120, 100, 121, 35, 117, 107, 114, 35, 119, 100, 122, 112, 35, 113, 119, 100, 122, 112, 35, 111, 120, 101, 35, 119, 118, 104, 121, 35, 119, 120, 35, 113, 104, 104, 106, 35, 111, 100, 120, 118, 35, 111, 114, 118, 35, 118, 108, 118, 35, 119, 118, 104, 121, 35, 113, 124, 114, 101, 35, 112, 120, 100, 109, 35, 110, 104, 121, 35, 119, 120, 35, 119, 107, 108, 100, 101, 35, 123, 124, 100, 35, 107, 113, 120, 101, 35, 120, 100, 35, 113, 119, 104, 109, 35, 124, 120, 100, 121, 35, 107, 111, 114, 114, 121, 35, 112, 120, 118, 35, 117, 100, 120, 35, 111, 122, 112, 35, 111, 120, 101, 35, 102, 107, 100, 121, 35, 113, 124, 114, 101, 35, 113, 124, 114, 101, 35, 117, 100, 120, 35, 111, 120, 101, 35, 119, 118, 104, 121, 35, 119, 120, 35, 113, 104, 104, 106, 35, 111, 100, 120, 118, 35, 111, 114, 118, 35, 118, 108, 118, 35, 119, 118, 104, 121, 35, 113, 124, 114, 101, 35, 112, 120, 100, 109, 35, 110, 104, 121, 35, 119, 120, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [85, 104, 118, 108, 103, 104, 113, 119, 118, 35, 112, 120, 118, 119, 35, 101, 104, 35, 113, 114, 119, 108, 105, 108, 104, 103, 47, 35, 108, 113, 35, 122, 117, 108, 119, 108, 113, 106, 47, 35, 114, 105, 35, 119, 107, 104, 35, 115, 117, 114, 115, 114, 118, 104, 103, 35, 103, 108, 118, 102, 107, 100, 117, 106, 104, 35, 114, 117, 35, 119, 117, 100, 113, 118, 105, 104, 117, 35, 100, 113, 103, 35, 108, 119, 118, 35, 109, 120, 118, 119, 108, 105, 108, 102, 100, 119, 108, 114, 113, 35, 113, 114, 35, 111, 100, 119, 104, 117, 35, 119, 107, 100, 113, 35, 54, 51, 35, 103, 100, 124, 118, 35, 101, 104, 105, 114, 117, 104, 35, 103, 108, 118, 102, 107, 100, 117, 106, 104, 35, 105, 117, 114, 112, 35, 119, 107, 104, 35, 113, 120, 117, 118, 108, 113, 106, 35, 114, 117, 35, 101, 114, 100, 117, 103, 108, 113, 106, 35, 102, 100, 117, 104, 35, 107, 114, 112, 104, 35, 100, 113, 103, 35, 118, 104, 121, 104, 113, 35, 103, 100, 124, 118, 35, 101, 104, 105, 114, 117, 104, 35, 119, 117, 100, 113, 118, 105, 104, 117, 35, 119, 114, 35, 100, 113, 114, 119, 107, 104, 117, 35, 117, 114, 114, 112, 35, 122, 108, 119, 107, 108, 113, 35, 119, 107, 104, 35, 113, 120, 117, 118, 108, 113, 106, 35, 114, 117, 35, 101, 114, 100, 117, 103, 108, 113, 106, 35, 102, 100, 117, 104, 35, 107, 114, 112, 104, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing tokenizer on inputs\n",
    "h_sentence = dataset[\"train\"][\"Hmong\"][1]\n",
    "eng_sentence = dataset[\"train\"][\"English\"][1]\n",
    "\n",
    "inputs = tokenizer(h_sentence, text_target=eng_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe75f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yuav tsum sau ntawv qhia cov neeg mob txog kev npaj rho tawm los sis hloov chaw thiab seb vim li cast sis pub dhau li 30 hnub ua ntej yuav rho tawm ntawm lub tsev tu neeg laus los sis tsev nyob muaj kev tu thiab xya hnub ua ntej yuav hloov mus rau lwm lub chav nyob nyob rau lub tsev tu neeg laus los sis tsev nyob muaj kev tu</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.decode(inputs['input_ids'])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a911c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f195f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Call for the model\n",
    "model_checkpoint = \"google/byt5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6a8c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19e956",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Decode token IDs to text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 (used to mask loss calculation) with padding token ID for decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Clean up spacing and format references properly\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    # Track average length of generated sequences\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # Round values for readability\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf64d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=True,\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=6,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0ecf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.\n",
      "  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())\n"
     ]
    },
    {
     "data": {
      "text/html": "\n    <div>\n      \n      <progress value='2' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/40 : < :, Epoch 0.02/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>",
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {
     },
     "output_type": "execute_result",
     "transient": {
      "display_id": "b7ef2df5c619773843a6eb86390cc498"
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.\n",
      "  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())\n"
     ]
    }
   ],
   "source": [
    "m3_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44932b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div>\n      \n      <progress value='1' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/61 : < :]\n    </div>\n    ",
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {
     },
     "output_type": "execute_result",
     "transient": {
      "display_id": "c162c391bda0de9e3f4612936968783d"
     }
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': nan,\n",
       " 'eval_bleu': 0.0222,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_runtime': 39.4921,\n",
       " 'eval_samples_per_second': 24.537,\n",
       " 'eval_steps_per_second': 1.545,\n",
       " 'epoch': 0.9917355371900827}"
      ]
     },
     "execution_count": 13,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b17c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ecc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "source_lang = \"English\"\n",
    "target_lang = \"Hmong\"\n",
    "max_target_length=128\n",
    "max_input_length=128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[source_lang]]\n",
    "    targets = [ex for ex in examples[target_lang]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['Hmong', 'English'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24f86d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Residents must be notified, in writing, of the proposed discharge or transfer and its justification no later than 30 days before discharge from the nursing or boarding care home and seven days before transfer to another room within the nursing or boarding care home</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing tokenizer on inputs\n",
    "h_sentence = dataset[\"train\"][\"Hmong\"][1]\n",
    "eng_sentence = dataset[\"train\"][\"English\"][1]\n",
    "\n",
    "inputs = tokenizer(eng_sentence, text_target=h_sentence)\n",
    "inputs\n",
    "\n",
    "output = tokenizer.decode(inputs['input_ids'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "973807",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdc0bc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=True,\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=6,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "771560",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.\n",
      "  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())\n"
     ]
    },
    {
     "data": {
      "text/html": "\n    <div>\n      \n      <progress value='2' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/40 : < :, Epoch 0.02/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>",
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {
     },
     "output_type": "execute_result",
     "transient": {
      "display_id": "7b5ad2ed453ade20cb691b262642e6f2"
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.\n",
      "  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())\n"
     ]
    },
    {
     "data": {
      "text/html": "\n    <div>\n      \n      <progress value='1' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/61 : < :]\n    </div>\n    ",
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {
     },
     "output_type": "execute_result",
     "transient": {
      "display_id": "71fa8db56a58a2d1fcbf4234ce81efe9"
     }
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': nan,\n",
       " 'eval_bleu': 0.0203,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_runtime': 41.4497,\n",
       " 'eval_samples_per_second': 23.378,\n",
       " 'eval_steps_per_second': 1.472,\n",
       " 'epoch': 0.9917355371900827}"
      ]
     },
     "execution_count": 18,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_results = trainer.train()\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/ext/venvs/cocalc/bin/python3",
    "-Xfrozen_modules=off",
    "-m",
    "ipykernel_launcher",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (CoCalc)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 system-wide environment. Use 'CoCalc' for the most complete Python environment.",
     "priority": 100,
     "url": "https://www.python.org/"
    },
    "debugger": true
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}